{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff9f46b5",
   "metadata": {},
   "source": [
    "# Random Forest classifier for Project AWARE\n",
    "\n",
    "This notebook follows the SRS and these steps:\n",
    "\n",
    "1. load data\n",
    "2. select features\n",
    "3. cleanup data\n",
    "4. split data into train and test\n",
    "5. create and train a Random Forest model\n",
    "6. evaluate and produce predictions (Low/Medium/High)\n",
    "\n",
    "The notebook will also include a blank input cell where you can enter values and get a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1224ceb5-3857-4c8d-9499-ece669402e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38c42a0-f8d9-4013-a9ed-fe098e4e1faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH = C:\\Users\\prana\\Documents\\PROJECTS\\AWARE\\extract\\WQ_combined_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths - set DATA_PATH to your CSV file path\n",
    "DATA_PATH = r\"\"\"C:\\Users\\prana\\Documents\\PROJECTS\\AWARE\\extract\\WQ_combined_clean.csv\"\"\"\n",
    "MODEL_OUTPUT = 'rf_aware_model.joblib'\n",
    "TARGET_COLUMN = None  # if you know the target column (e.g., 'Risk'), set it here\n",
    "\n",
    "print('DATA_PATH =',r\"C:\\Users\\prana\\Documents\\PROJECTS\\AWARE\\extract\\WQ_combined_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db34ca9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load data\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print('Loaded dataset with shape:', df.shape)\n",
    "    display(df.head())\n",
    "except Exception as e:\n",
    "    print('Error loading CSV. Please check DATA_PATH. Error:', e)\n",
    "    df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0626b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Select features\n",
    "if df is None:\n",
    "    print('Dataset not loaded; set DATA_PATH and re-run.')\n",
    "else:\n",
    "    # Auto-detect target column\n",
    "    if TARGET_COLUMN:\n",
    "        target_col = TARGET_COLUMN\n",
    "    else:\n",
    "        possible = [c for c in df.columns if c.lower()=='risk' or c.lower()=='risk_level' or c.lower()=='label']\n",
    "        if len(possible)>0:\n",
    "            target_col = possible[0]\n",
    "        else:\n",
    "            proxies = ['cases','case_count','diarrhea_cases','diarrhoea_cases','turbidity','ph']\n",
    "            found = [c for c in df.columns if c.lower() in proxies]\n",
    "            target_col = None\n",
    "            if found:\n",
    "                print('No explicit target column found. You have columns that might be proxies:', found)\n",
    "                print('You should set TARGET_COLUMN manually. For now notebook will proceed but will NOT train a classifier until a target is specified.')\n",
    "    print('TARGET_COLUMN =', target_col)\n",
    "\n",
    "    # Choose numeric features by default (exclude target)\n",
    "    if target_col and target_col in df.columns:\n",
    "        feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c!=target_col]\n",
    "    else:\n",
    "        feature_cols = list(df.select_dtypes(include=[np.number]).columns)\n",
    "    print('Selected feature columns (numeric):', feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84210f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Cleanup data\n",
    "if df is not None:\n",
    "    X = df[feature_cols].copy()\n",
    "    # Impute numeric missing values with median\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    print('Missing values per column after imputation:')\n",
    "    print(X_imputed.isna().sum())\n",
    "\n",
    "    # Handle target encoding if present\n",
    "    y = None\n",
    "    if 'target_col' in globals() and target_col and target_col in df.columns:\n",
    "        y = df[target_col].copy()\n",
    "        if y.dtype == object or y.dtype.name == 'category':\n",
    "            le = LabelEncoder()\n",
    "            y = le.fit_transform(y.astype(str))\n",
    "            print('Encoded target classes:', list(le.classes_))\n",
    "        else:\n",
    "            # if numeric continuous target, try to bin into Low/Medium/High\n",
    "            if y.nunique()>3:\n",
    "                y_binned = pd.qcut(y, q=3, labels=['Low','Medium','High'])\n",
    "                le = LabelEncoder()\n",
    "                y = le.fit_transform(y_binned.astype(str))\n",
    "                print('Binned continuous target into classes:', list(le.classes_))\n",
    "            else:\n",
    "                print('Numeric target with few unique values; used as-is.')\n",
    "    else:\n",
    "        print('No target found yet. Set TARGET_COLUMN and re-run if you want to train.')\n",
    "\n",
    "    X_processed = X_imputed\n",
    "    y_processed = y\n",
    "else:\n",
    "    print('No dataframe loaded; cannot clean data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Split and 5) Train Random Forest\n",
    "if df is not None and y_processed is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed)\n",
    "    print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "\n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Model trained.')\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('\\nClassification report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Save model and imputer and label encoder (if exists)\n",
    "    joblib.dump({'model': rf, 'imputer': imputer, 'label_encoder': globals().get('le', None)}, MODEL_OUTPUT)\n",
    "    print('Saved model object to', MODEL_OUTPUT)\n",
    "else:\n",
    "    print('Either dataset not loaded or target not found. If you want to train, set TARGET_COLUMN to the name of the target column (e.g., \"Risk\") and re-run.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Prediction on blank input\n",
    "# This cell provides a BLANK_INPUT where you should fill a dictionary with feature values (numeric) and run the cell to get a prediction.\n",
    "\n",
    "BLANK_INPUT = {\n",
    "    # Example: 'turbidity': 5.2, 'ph': 7.1, 'case_count': 2\n",
    "    # <-- Replace keys/values with the exact feature columns listed earlier.\n",
    "}\n",
    "\n",
    "if BLANK_INPUT:\n",
    "    # Load model\n",
    "    obj = joblib.load(MODEL_OUTPUT)\n",
    "    model = obj['model']\n",
    "    imputer = obj['imputer']\n",
    "    le = obj.get('label_encoder', None)\n",
    "\n",
    "    # Create DataFrame from BLANK_INPUT using feature_cols order\n",
    "    x_new = pd.DataFrame([BLANK_INPUT])\n",
    "    # Align columns\n",
    "    x_new = x_new.reindex(columns=X_processed.columns)\n",
    "    # Impute\n",
    "    x_new_imputed = pd.DataFrame(imputer.transform(x_new), columns=x_new.columns)\n",
    "    pred = model.predict(x_new_imputed)\n",
    "    if le is not None:\n",
    "        try:\n",
    "            pred_label = le.inverse_transform(pred)\n",
    "        except Exception:\n",
    "            pred_label = pred\n",
    "    else:\n",
    "        pred_label = pred\n",
    "    print('Prediction (encoded):', pred)\n",
    "    print('Prediction (label):', pred_label)\n",
    "else:\n",
    "    print('BLANK_INPUT is empty. Fill BLANK_INPUT with feature values and re-run this cell to get a prediction.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846170c",
   "metadata": {},
   "source": [
    "## Next steps / Notes\n",
    "\n",
    "- Open this notebook on your machine (Jupyter / JupyterLab / VS Code) and set `DATA_PATH` if it's different.\n",
    "- If the dataset already contains a `Risk` (Low/Medium/High) column, set `TARGET_COLUMN='Risk'` to train directly.\n",
    "- If target isn't present, decide which column to use as a proxy (e.g., case counts or an existing risk score) and set `TARGET_COLUMN`.\n",
    "- Customize feature selection if you want to include categorical columns (the current notebook uses only numeric features by default).\n",
    "\n",
    "If you want, I can also: \n",
    "- return a Python script (.py) instead, or\n",
    "- run the training here and show evaluation results (if you upload the CSV to this environment)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
