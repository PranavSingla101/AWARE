{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b338babf",
   "metadata": {},
   "source": [
    "# AWARE — Random Forest (updated for `water_dataX.csv`)\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Loads `/mnt/data/water_dataX.csv`\n",
    "2. Cleans & renames columns\n",
    "3. Creates `Risk` column using WHO/BIS-aligned thresholds\n",
    "4. Trains a Random Forest classifier\n",
    "5. Saves the trained model and imputer\n",
    "6. Provides a `BLANK_INPUT` cell to make predictions on new data\n",
    "\n",
    "Run cells top-to-bottom. If your CSV is elsewhere, change `DATA_PATH` in the first code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "402e7304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH = /Users/anishsharma/Developer/SE project/extract/water_dataX.csv\n",
      "MODEL_OUTPUT = /Users/anishsharma/Developer/SE project/extract/rf_water_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Cell: imports and settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "# --- DEFAULT PATHS ---\n",
    "WINDOWS_DATA = Path(r\"C:\\Users\\prana\\Documents\\PROJECTS\\AWARE\\extract\\water_dataX.csv\")\n",
    "WINDOWS_MODEL = Path(r\"C:\\Users\\prana\\Documents\\PROJECTS\\AWARE\\extract\\rf_water_model.joblib\")\n",
    "\n",
    "MAC_DATA = Path(\"/Users/anishsharma/Developer/SE project/extract/water_dataX.csv\")\n",
    "MAC_MODEL = Path(\"/Users/anishsharma/Developer/SE project/extract/rf_water_model.joblib\")\n",
    "\n",
    "# Allow environment override\n",
    "ENV_DATA = os.getenv(\"AWARE_DATA_PATH\")\n",
    "ENV_MODEL = os.getenv(\"AWARE_MODEL_OUTPUT\")\n",
    "\n",
    "# --- PICK DATA_PATH WITH TRY LOGIC ---\n",
    "if ENV_DATA:\n",
    "    DATA_PATH = Path(ENV_DATA)\n",
    "elif WINDOWS_DATA.exists():\n",
    "    DATA_PATH = WINDOWS_DATA\n",
    "elif MAC_DATA.exists():\n",
    "    DATA_PATH = MAC_DATA\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find dataset.\\nTried:\\n  {WINDOWS_DATA}\\n  {MAC_DATA}\\n\"\n",
    "    )\n",
    "\n",
    "# --- FIXED MODEL_OUTPUT LOGIC ---\n",
    "if ENV_MODEL:\n",
    "    MODEL_OUTPUT = Path(ENV_MODEL)\n",
    "elif os.name == \"nt\":  # Windows\n",
    "    MODEL_OUTPUT = WINDOWS_MODEL\n",
    "else:                  # macOS or Linux\n",
    "    MODEL_OUTPUT = MAC_MODEL\n",
    "\n",
    "print(\"DATA_PATH =\", DATA_PATH)\n",
    "print(\"MODEL_OUTPUT =\", MODEL_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af09be56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8 failed, trying latin1…\n",
      "Loaded shape: (1991, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION CODE</th>\n",
       "      <th>LOCATIONS</th>\n",
       "      <th>STATE</th>\n",
       "      <th>Temp</th>\n",
       "      <th>D.O. (mg/l)</th>\n",
       "      <th>PH</th>\n",
       "      <th>CONDUCTIVITY (µmhos/cm)</th>\n",
       "      <th>B.O.D. (mg/l)</th>\n",
       "      <th>NITRATENAN N+ NITRITENANN (mg/l)</th>\n",
       "      <th>FECAL COLIFORM (MPN/100ml)</th>\n",
       "      <th>TOTAL COLIFORM (MPN/100ml)Mean</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1393</td>\n",
       "      <td>DAMANGANGA AT D/S OF MADHUBAN, DAMAN</td>\n",
       "      <td>DAMAN &amp; DIU</td>\n",
       "      <td>30.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7.5</td>\n",
       "      <td>203</td>\n",
       "      <td>NAN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1399</td>\n",
       "      <td>ZUARI AT D/S OF PT. WHERE KUMBARJRIA CANAL JOI...</td>\n",
       "      <td>GOA</td>\n",
       "      <td>29.8</td>\n",
       "      <td>5.7</td>\n",
       "      <td>7.2</td>\n",
       "      <td>189</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4953</td>\n",
       "      <td>8391</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1475</td>\n",
       "      <td>ZUARI AT PANCHAWADI</td>\n",
       "      <td>GOA</td>\n",
       "      <td>29.5</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>179</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3243</td>\n",
       "      <td>5330</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3181</td>\n",
       "      <td>RIVER ZUARI AT BORIM BRIDGE</td>\n",
       "      <td>GOA</td>\n",
       "      <td>29.7</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.9</td>\n",
       "      <td>64</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5382</td>\n",
       "      <td>8443</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3182</td>\n",
       "      <td>RIVER ZUARI AT MARCAIM JETTY</td>\n",
       "      <td>GOA</td>\n",
       "      <td>29.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>83</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3428</td>\n",
       "      <td>5500</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  STATION CODE                                          LOCATIONS  \\\n",
       "0         1393               DAMANGANGA AT D/S OF MADHUBAN, DAMAN   \n",
       "1         1399  ZUARI AT D/S OF PT. WHERE KUMBARJRIA CANAL JOI...   \n",
       "2         1475                                ZUARI AT PANCHAWADI   \n",
       "3         3181                        RIVER ZUARI AT BORIM BRIDGE   \n",
       "4         3182                       RIVER ZUARI AT MARCAIM JETTY   \n",
       "\n",
       "         STATE  Temp D.O. (mg/l)   PH CONDUCTIVITY (µmhos/cm) B.O.D. (mg/l)  \\\n",
       "0  DAMAN & DIU  30.6         6.7  7.5                     203           NAN   \n",
       "1          GOA  29.8         5.7  7.2                     189             2   \n",
       "2          GOA  29.5         6.3  6.9                     179           1.7   \n",
       "3          GOA  29.7         5.8  6.9                      64           3.8   \n",
       "4          GOA  29.5         5.8  7.3                      83           1.9   \n",
       "\n",
       "  NITRATENAN N+ NITRITENANN (mg/l) FECAL COLIFORM (MPN/100ml)  \\\n",
       "0                              0.1                         11   \n",
       "1                              0.2                       4953   \n",
       "2                              0.1                       3243   \n",
       "3                              0.5                       5382   \n",
       "4                              0.4                       3428   \n",
       "\n",
       "  TOTAL COLIFORM (MPN/100ml)Mean  year  \n",
       "0                             27  2014  \n",
       "1                           8391  2014  \n",
       "2                           5330  2014  \n",
       "3                           8443  2014  \n",
       "4                           5500  2014  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1991 entries, 0 to 1990\n",
      "Data columns (total 12 columns):\n",
      " #   Column                            Non-Null Count  Dtype \n",
      "---  ------                            --------------  ----- \n",
      " 0   STATION CODE                      1991 non-null   object\n",
      " 1   LOCATIONS                         1991 non-null   object\n",
      " 2   STATE                             1991 non-null   object\n",
      " 3   Temp                              1991 non-null   object\n",
      " 4   D.O. (mg/l)                       1991 non-null   object\n",
      " 5   PH                                1991 non-null   object\n",
      " 6   CONDUCTIVITY (µmhos/cm)           1991 non-null   object\n",
      " 7   B.O.D. (mg/l)                     1991 non-null   object\n",
      " 8   NITRATENAN N+ NITRITENANN (mg/l)  1991 non-null   object\n",
      " 9   FECAL COLIFORM (MPN/100ml)        1991 non-null   object\n",
      " 10  TOTAL COLIFORM (MPN/100ml)Mean    1991 non-null   object\n",
      " 11  year                              1991 non-null   int64 \n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 186.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Cell: load data\n",
    "# Try UTF-8 first (recommended)\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, encoding='utf-8', low_memory=False)\n",
    "except UnicodeDecodeError:\n",
    "    print(\"UTF-8 failed, trying latin1…\")\n",
    "    df = pd.read_csv(DATA_PATH, encoding='latin1', low_memory=False)\n",
    "\n",
    "# Optional: parse dates after load\n",
    "if 'sample_date' in df.columns:\n",
    "    df['sample_date'] = pd.to_datetime(df['sample_date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "display(df.head())\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2d5c3fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied column renames (sample):\n",
      "  STATION CODE -> StationCode\n",
      "  LOCATIONS -> MonitoringLocation\n",
      "  STATE -> State\n",
      "  Temp -> Temp\n",
      "  D.O. (mg/l) -> DO\n",
      "  PH -> pH\n",
      "  CONDUCTIVITY (µmhos/cm) -> Conductivity\n",
      "  B.O.D. (mg/l) -> BOD\n",
      "  NITRATENAN N+ NITRITENANN (mg/l) -> Nitrate\n",
      "  FECAL COLIFORM (MPN/100ml) -> FecalColiform\n",
      "  TOTAL COLIFORM (MPN/100ml)Mean -> TotalColiform\n",
      "  year -> Year\n",
      "\n",
      "Numeric columns summary:\n",
      " Temp: non-null after coercion = 1899 (coerced_to_nan = 92)\n",
      " DO: non-null after coercion = 1960 (coerced_to_nan = 31)\n",
      " pH: non-null after coercion = 1983 (coerced_to_nan = 8)\n",
      " Conductivity: non-null after coercion = 1966 (coerced_to_nan = 25)\n",
      " BOD: non-null after coercion = 1948 (coerced_to_nan = 43)\n",
      " Nitrate: non-null after coercion = 1766 (coerced_to_nan = 225)\n",
      " FecalColiform: non-null after coercion = 1675 (coerced_to_nan = 316)\n",
      " TotalColiform: non-null after coercion = 1859 (coerced_to_nan = 132)\n",
      "\n",
      "Warning: 89 pH values outside 0-14 range (these will remain in df as-is).\n",
      "\n",
      "After cleaning shape: (1991, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp</th>\n",
       "      <th>DO</th>\n",
       "      <th>pH</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>BOD</th>\n",
       "      <th>Nitrate</th>\n",
       "      <th>FecalColiform</th>\n",
       "      <th>TotalColiform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7.5</td>\n",
       "      <td>203.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.8</td>\n",
       "      <td>5.7</td>\n",
       "      <td>7.2</td>\n",
       "      <td>189.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4953.0</td>\n",
       "      <td>8391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.5</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>179.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3243.0</td>\n",
       "      <td>5330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.7</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.9</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5382.0</td>\n",
       "      <td>8443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3428.0</td>\n",
       "      <td>5500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temp   DO   pH  Conductivity  BOD  Nitrate  FecalColiform  TotalColiform\n",
       "0  30.6  6.7  7.5         203.0  NaN      0.1           11.0           27.0\n",
       "1  29.8  5.7  7.2         189.0  2.0      0.2         4953.0         8391.0\n",
       "2  29.5  6.3  6.9         179.0  1.7      0.1         3243.0         5330.0\n",
       "3  29.7  5.8  6.9          64.0  3.8      0.5         5382.0         8443.0\n",
       "4  29.5  5.8  7.3          83.0  1.9      0.4         3428.0         5500.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell: clean & rename columns (improved)\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def normalize_colname(s):\n",
    "    \"\"\"Normalize a column name to a simple form for matching.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    # remove BOM/newlines/tabs, collapse spaces, strip\n",
    "    s = s.replace('\\ufeff', '').replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s.lower()\n",
    "\n",
    "if df is not None:\n",
    "    # canonical rename map (lowercase keys)\n",
    "    rename_map = {\n",
    "        'temp': 'Temp',\n",
    "        'd.o. (mg/l)': 'DO',  # keep original patterns if present\n",
    "        'd.o. (mg/l)': 'DO',\n",
    "        'ph': 'pH',\n",
    "        'conductivity (µmhos/cm)': 'Conductivity',\n",
    "        'b.o.d. (mg/l)': 'BOD',\n",
    "        'nitrate': 'Nitrate',\n",
    "        'fecal coliform (mpn/100ml)': 'FecalColiform',\n",
    "        'total coliform (mpn/100ml)': 'TotalColiform',\n",
    "        'year': 'Year',\n",
    "        'station code': 'StationCode',\n",
    "        'locations': 'MonitoringLocation',\n",
    "        'state': 'State'\n",
    "    }\n",
    "\n",
    "    # Build a normalized -> original column mapping\n",
    "    orig_cols = list(df.columns)\n",
    "    norm_map = {orig: normalize_colname(orig) for orig in orig_cols}\n",
    "\n",
    "    # Attempt mapping using exact normalized matches first\n",
    "    col_map = {}\n",
    "    for orig, norm in norm_map.items():\n",
    "        if norm in rename_map:\n",
    "            col_map[orig] = rename_map[norm]\n",
    "        else:\n",
    "            # heuristic substring matches for common patterns (nitrate, ph, conductivity, coliform, etc.)\n",
    "            if 'nitrate' in norm or 'nitrat' in norm or 'no3' in norm:\n",
    "                col_map[orig] = 'Nitrate'\n",
    "            elif 'coliform' in norm and 'fecal' in norm:\n",
    "                col_map[orig] = 'FecalColiform'\n",
    "            elif 'total coliform' in norm or (('coliform' in norm) and ('total' in norm)):\n",
    "                col_map[orig] = 'TotalColiform'\n",
    "            elif 'conduct' in norm:\n",
    "                col_map[orig] = 'Conductivity'\n",
    "            elif norm.startswith('ph') or norm == 'ph':\n",
    "                col_map[orig] = 'pH'\n",
    "            elif norm.startswith('b.o.d') or 'bod' in norm:\n",
    "                col_map[orig] = 'BOD'\n",
    "            elif norm.startswith('d.o') or 'do ' in norm:\n",
    "                col_map[orig] = 'DO'\n",
    "            # you can add more heuristics here\n",
    "            else:\n",
    "                # optional: fuzzy match to known keys (helpful for typos)\n",
    "                matches = get_close_matches(norm, list(rename_map.keys()), n=1, cutoff=0.8)\n",
    "                if matches:\n",
    "                    col_map[orig] = rename_map[matches[0]]\n",
    "                # else: leave unmapped\n",
    "\n",
    "    # Apply renaming\n",
    "    df = df.rename(columns=col_map)\n",
    "    print(\"Applied column renames (sample):\")\n",
    "    for k, v in list(col_map.items())[:20]:\n",
    "        print(f\"  {k} -> {v}\")\n",
    "\n",
    "    # Standardize whitespace in column names (safe canonical names)\n",
    "    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n",
    "\n",
    "    # Expected numeric columns (canonical names)\n",
    "    numeric_cols = ['Temp', 'DO', 'pH', 'Conductivity', 'BOD', 'Nitrate', 'FecalColiform', 'TotalColiform']\n",
    "\n",
    "    # Pre-clean numeric text function\n",
    "    def clean_numeric_series(s):\n",
    "        # convert to string, remove commas, remove unit strings and footnote markers, convert >, < to numeric approx\n",
    "        s = s.astype(str).str.strip()\n",
    "        # handle '>' and '<' by removing symbol (could also prefix a flag if needed)\n",
    "        s = s.str.replace(r'[<>]', '', regex=True)\n",
    "        # remove commas and common non-numeric chars (units)\n",
    "        s = s.str.replace(r',', '', regex=True)\n",
    "        s = s.str.replace(r'[^\\d\\.\\-eE]', '', regex=True)  # keep digits, dot, minus, exponent\n",
    "        # empty strings -> NaN\n",
    "        s = s.replace({'': np.nan, 'nan': np.nan, 'None': np.nan})\n",
    "        return s\n",
    "\n",
    "    # Create missing-indicator dictionary and coerce\n",
    "    missing_report = {}\n",
    "    for c in numeric_cols:\n",
    "        if c in df.columns:\n",
    "            before_nonnull = df[c].notna().sum()\n",
    "            # clean then coerce\n",
    "            df[c] = clean_numeric_series(df[c])\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "            after_nonnull = df[c].notna().sum()\n",
    "            coerced_to_nan = before_nonnull - after_nonnull\n",
    "            missing_report[c] = {'present': True, 'coerced_to_nan': coerced_to_nan, 'non_null_after': after_nonnull}\n",
    "        else:\n",
    "            # column missing entirely\n",
    "            df[c] = np.nan\n",
    "            missing_report[c] = {'present': False, 'coerced_to_nan': None, 'non_null_after': 0}\n",
    "\n",
    "    # Print missing / coercion summary\n",
    "    print(\"\\nNumeric columns summary:\")\n",
    "    for c, info in missing_report.items():\n",
    "        if info['present']:\n",
    "            print(f\" {c}: non-null after coercion = {info['non_null_after']} (coerced_to_nan = {info['coerced_to_nan']})\")\n",
    "        else:\n",
    "            print(f\" {c}: MISSING (created column filled with NaN)\")\n",
    "\n",
    "    # Optional: flag suspicious values (simple example)\n",
    "    if 'pH' in df.columns:\n",
    "        bad_ph = df.loc[~df['pH'].between(0, 14) & df['pH'].notna(), 'pH']\n",
    "        if len(bad_ph) > 0:\n",
    "            print(f\"\\nWarning: {len(bad_ph)} pH values outside 0-14 range (these will remain in df as-is).\")\n",
    "\n",
    "    # Drop rows with no StationCode AND all numeric cols missing (your previous logic)\n",
    "    if 'StationCode' in df.columns:\n",
    "        df = df[df['StationCode'].notna()]\n",
    "\n",
    "    # Show shape and head\n",
    "    df = df.reset_index(drop=True)\n",
    "    print('\\nAfter cleaning shape:', df.shape)\n",
    "    display(df[numeric_cols].head())\n",
    "\n",
    "else:\n",
    "    print('Dataset not loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d117cc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-parameter risk counts (score):\n",
      " pH: {'Low': 1259, 'Medium': 508, 'High': 216, 'NaN': 8}\n",
      " DO: {'Low': 1729, 'Medium': 172, 'High': 59, 'NaN': 31}\n",
      " BOD: {'Low': 406, 'Medium': 930, 'High': 612, 'NaN': 43}\n",
      " Conductivity: {'Low': 1428, 'Medium': 293, 'High': 245, 'NaN': 25}\n",
      " Nitrate: {'Low': 1711, 'Medium': 53, 'High': 2, 'NaN': 225}\n",
      " TotalColiform: {'Low': 951, 'Medium': 502, 'High': 406, 'NaN': 132}\n",
      " FecalColiform: {'Low': 647, 'Medium': 482, 'High': 546, 'NaN': 316}\n",
      "\n",
      "Overall Risk distribution (including NaN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Risk\n",
       "High      1203\n",
       "Medium     714\n",
       "Low         70\n",
       "NaN          4\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 89 pH values outside 0-14 range. Inspect these rows manually.\n"
     ]
    }
   ],
   "source": [
    "# Cell: create WHO-based Risk column (improved, vectorized, configurable)\n",
    "import numpy as np\n",
    "\n",
    "if df is not None:\n",
    "    # --- Configurable thresholds (document units) ---\n",
    "    # Note: all thresholds assume units are consistent with cleaned numeric columns.\n",
    "    # e.g. pH (unitless), DO (mg/L), BOD (mg/L), Conductivity (µS/cm), Nitrate (mg/L),\n",
    "    # TotalColiform (MPN/100 mL), FecalColiform (MPN/100 mL)\n",
    "    THRESHOLDS = {\n",
    "        'pH':        {'low_high': (6.5, 8.5), 'medium_ranges': [(6.5, 7.0), (8.0, 8.5)]},  # keep for doc\n",
    "        'DO':        {'high': 3.0, 'medium': 5.0},            # <3 -> High, <5 -> Medium, else Low\n",
    "        'BOD':       {'medium': 1.0, 'high': 3.0},            # >3 High, >1 Medium\n",
    "        'Conductivity': {'medium': 500, 'high': 1500},       # >1500 High, >500 Medium\n",
    "        'Nitrate':   {'medium': 10, 'high': 45},             # >45 High, >10 Medium\n",
    "        'TotalColiform': {'medium': 500, 'high': 2500},      # >2500 High, >500 Medium\n",
    "        'FecalColiform': {'medium': 100, 'high': 500}        # >500 High, >100 Medium\n",
    "    }\n",
    "\n",
    "    # risk score mapping and inverse\n",
    "    RISK_TO_SCORE = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    SCORE_TO_RISK = {v: k for k, v in RISK_TO_SCORE.items()}\n",
    "\n",
    "    # helper to safely get series or a nan-series if missing\n",
    "    def get_series(col):\n",
    "        if col in df.columns:\n",
    "            return df[col]\n",
    "        else:\n",
    "            # create column of NaNs to avoid KeyError; we'll log later\n",
    "            df[col] = np.nan\n",
    "            return df[col]\n",
    "\n",
    "    # create missing-indicator columns & ensure numeric dtype\n",
    "    params = ['pH', 'DO', 'BOD', 'Conductivity', 'Nitrate', 'TotalColiform', 'FecalColiform']\n",
    "    for p in params:\n",
    "        if p not in df.columns:\n",
    "            print(f\"Warning: column '{p}' not found. Creating as NaN.\")\n",
    "            df[p] = np.nan\n",
    "        # missing indicator\n",
    "        df[f'{p}_missing'] = df[p].isna().astype(int)\n",
    "\n",
    "    # Vectorized risk assignment - produce numeric score columns (0,1,2) quickly\n",
    "    # pH is special because both low and high extremes are risky\n",
    "    # Initialize score columns with NaN\n",
    "    for p in params:\n",
    "        df[f'{p}_score'] = np.nan\n",
    "\n",
    "    # pH handling (two-sided)\n",
    "    if 'pH' in THRESHOLDS:\n",
    "        col = df['pH']\n",
    "        # High: pH < 6.5 or pH > 8.5\n",
    "        high_mask = col < 6.5\n",
    "        high_mask |= col > 8.5\n",
    "        # Medium: (6.5 <= pH < 7.0) or (8.0 < pH <= 8.5)\n",
    "        medium_mask = ((col >= 6.5) & (col < 7.0)) | ((col > 8.0) & (col <= 8.5))\n",
    "        low_mask = (~high_mask) & (~medium_mask) & (col.notna())\n",
    "        df.loc[high_mask, 'pH_score'] = RISK_TO_SCORE['High']\n",
    "        df.loc[medium_mask, 'pH_score'] = RISK_TO_SCORE['Medium']\n",
    "        df.loc[low_mask, 'pH_score'] = RISK_TO_SCORE['Low']\n",
    "\n",
    "    # generic pattern for parameters where \">\" means worse (e.g., BOD, Conductivity, Nitrate, Coliforms)\n",
    "    for p in ['DO', 'BOD', 'Conductivity', 'Nitrate', 'TotalColiform', 'FecalColiform']:\n",
    "        s = df[p]\n",
    "        t = THRESHOLDS.get(p, {})\n",
    "        if p == 'DO':\n",
    "            # for DO lower is worse\n",
    "            high_mask = s < t['high']   # <3 -> High\n",
    "            medium_mask = (s < t['medium']) & (~high_mask)  # <5 and >=3 -> Medium\n",
    "            low_mask = (~high_mask) & (~medium_mask) & (s.notna())\n",
    "        else:\n",
    "            high_mask = s > t['high']\n",
    "            medium_mask = (s > t['medium']) & (~high_mask)\n",
    "            low_mask = (~high_mask) & (~medium_mask) & (s.notna())\n",
    "\n",
    "        df.loc[high_mask, f'{p}_score'] = RISK_TO_SCORE['High']\n",
    "        df.loc[medium_mask, f'{p}_score'] = RISK_TO_SCORE['Medium']\n",
    "        df.loc[low_mask, f'{p}_score'] = RISK_TO_SCORE['Low']\n",
    "\n",
    "    # Log parameter-level distributions\n",
    "    print(\"Per-parameter risk counts (score):\")\n",
    "    for p in params:\n",
    "        counts = df[f'{p}_score'].value_counts(dropna=False).sort_index()\n",
    "        # map index numbers back to risk labels for readability\n",
    "        readable = {SCORE_TO_RISK.get(int(k), 'NaN') if not np.isnan(k) else 'NaN': v for k, v in counts.items()}\n",
    "        print(f\" {p}: {readable}\")\n",
    "\n",
    "    # --- Combine into overall risk ---\n",
    "    # approach: take the maximum score across parameters (worst-case); alternative: weighted max\n",
    "    score_cols = [f'{p}_score' for p in params]\n",
    "    # convert to numeric and ignore NaNs for max\n",
    "    df['overall_score'] = df[score_cols].max(axis=1, skipna=True)\n",
    "\n",
    "    # If all score columns are NaN for a row, overall_score will be NaN -> keep as NaN (no data)\n",
    "    # Map back to labels\n",
    "    df['Risk'] = df['overall_score'].map(SCORE_TO_RISK)\n",
    "\n",
    "    # Optional: if you want to require at least N non-missing parameter scores to set overall risk:\n",
    "    # df['non_missing_scores'] = df[score_cols].notna().sum(axis=1)\n",
    "    # df.loc[df['non_missing_scores'] < 2, 'Risk'] = np.nan  # require at least 2 params present (example)\n",
    "\n",
    "    # Print distribution\n",
    "    print(\"\\nOverall Risk distribution (including NaN):\")\n",
    "    display(df['Risk'].value_counts(dropna=False))\n",
    "\n",
    "    # Optional: flag suspicious values\n",
    "    bad_ph = df.loc[~df['pH'].between(0, 14) & df['pH'].notna(), 'pH']\n",
    "    if len(bad_ph) > 0:\n",
    "        print(f\"Warning: {len(bad_ph)} pH values outside 0-14 range. Inspect these rows manually.\")\n",
    "\n",
    "else:\n",
    "    print('Dataset not loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1f143558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-parameter risk (score) counts:\n",
      "  pH: {'Low': np.int64(1259), 'Medium': np.int64(508), 'High': np.int64(216), 'NaN': np.int64(8)}\n",
      "  DO: {'Low': np.int64(1729), 'Medium': np.int64(172), 'High': np.int64(59), 'NaN': np.int64(31)}\n",
      "  BOD: {'Low': np.int64(406), 'Medium': np.int64(930), 'High': np.int64(612), 'NaN': np.int64(43)}\n",
      "  Conductivity: {'Low': np.int64(1428), 'Medium': np.int64(293), 'High': np.int64(245), 'NaN': np.int64(25)}\n",
      "  Nitrate: {'Low': np.int64(1711), 'Medium': np.int64(53), 'High': np.int64(2), 'NaN': np.int64(225)}\n",
      "  TotalColiform: {'Low': np.int64(951), 'Medium': np.int64(502), 'High': np.int64(406), 'NaN': np.int64(132)}\n",
      "  FecalColiform: {'Low': np.int64(647), 'Medium': np.int64(482), 'High': np.int64(546), 'NaN': np.int64(316)}\n",
      "\n",
      "Overall Risk distribution (including NaN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Risk\n",
       "High      1203\n",
       "Medium     714\n",
       "Low         70\n",
       "NaN          4\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: BOD marked High in 31.4% of rows — verify units/thresholds.\n",
      "Warning: FecalColiform marked High in 32.6% of rows — verify units/thresholds.\n",
      "Warning: 89 pH values outside 0-14 range. Inspect these rows manually.\n",
      "\n",
      "Risk computation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell: create WHO-based Risk column (single copy-paste updated cell)\n",
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "# Documented thresholds (units must match cleaned numeric columns)\n",
    "THRESHOLDS = {\n",
    "    'pH':            {'low': 6.5, 'high': 8.5, 'medium_low': 6.5, 'medium_high': 7.0, 'medium2_low': 8.0, 'medium2_high': 8.5},\n",
    "    'DO':            {'high': 3.0, 'medium': 5.0},            # DO (mg/L): <3 High, <5 Medium, else Low\n",
    "    'BOD':           {'medium': 1.0, 'high': 3.0},            # BOD (mg/L): >3 High, >1 Medium\n",
    "    'Conductivity':  {'medium': 500, 'high': 1500},          # µS/cm or µmhos/cm\n",
    "    'Nitrate':       {'medium': 10, 'high': 45},             # mg/L\n",
    "    'TotalColiform': {'medium': 500, 'high': 2500},          # MPN/100 mL\n",
    "    'FecalColiform': {'medium': 100, 'high': 500}            # MPN/100 mL\n",
    "}\n",
    "\n",
    "# Controls\n",
    "RISK_TO_SCORE = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "SCORE_TO_RISK = {v: k for k, v in RISK_TO_SCORE.items()}\n",
    "PARAMS = ['pH', 'DO', 'BOD', 'Conductivity', 'Nitrate', 'TotalColiform', 'FecalColiform']\n",
    "MIN_NON_MISSING = 2   # require at least this many parameter scores to set an overall Risk\n",
    "AGG_METHOD = 'max'    # 'max' (default worst-case) or 'weighted' (see WEIGHTS)\n",
    "WEIGHTS = {'pH':1.0, 'DO':1.0, 'BOD':1.0, 'Conductivity':0.5, 'Nitrate':0.8, 'TotalColiform':1.0, 'FecalColiform':1.2}\n",
    "MISSING_WARNING_PCT = 0.5    # warn if param missing in >50% rows\n",
    "HIGH_PCT_THRESHOLD = 0.3     # warn if param High in >30% rows\n",
    "KEEP_INTERMEDIATE = True     # set False to drop *_score and *_missing columns after\n",
    "\n",
    "# --- Implementation ---\n",
    "if df is None:\n",
    "    print(\"Dataset not loaded (df is None).\")\n",
    "else:\n",
    "    # Ensure required columns exist (create as NaN and warn if missing)\n",
    "    for p in PARAMS:\n",
    "        if p not in df.columns:\n",
    "            print(f\"Warning: column '{p}' not found. Creating as NaN.\")\n",
    "            df[p] = np.nan\n",
    "        # missing indicator\n",
    "        df[f'{p}_missing'] = df[p].isna().astype('int8')\n",
    "\n",
    "    # Initialize score columns as float (NaN-able) and then convert to nullable Int\n",
    "    for p in PARAMS:\n",
    "        df[f'{p}_score'] = np.nan\n",
    "\n",
    "    # --- pH (two-sided) ---\n",
    "    p = 'pH'\n",
    "    if p in THRESHOLDS:\n",
    "        col = df[p]\n",
    "        low_thresh, high_thresh = THRESHOLDS[p]['low'], THRESHOLDS[p]['high']\n",
    "        # High if outside [low_thresh, high_thresh]\n",
    "        high_mask = (col < low_thresh) | (col > high_thresh)\n",
    "        medium_mask = (((col >= THRESHOLDS[p]['medium_low']) & (col < THRESHOLDS[p]['medium_high'])) |\n",
    "                       ((col > THRESHOLDS[p]['medium2_low']) & (col <= THRESHOLDS[p]['medium2_high'])))\n",
    "        low_mask = (~high_mask) & (~medium_mask) & col.notna()\n",
    "        df.loc[high_mask, f'{p}_score'] = RISK_TO_SCORE['High']\n",
    "        df.loc[medium_mask, f'{p}_score'] = RISK_TO_SCORE['Medium']\n",
    "        df.loc[low_mask, f'{p}_score'] = RISK_TO_SCORE['Low']\n",
    "\n",
    "    # --- Generic rules for other params (direction: higher is worse except DO) ---\n",
    "    for p in [x for x in PARAMS if x != 'pH']:\n",
    "        s = df[p]\n",
    "        t = THRESHOLDS.get(p, {})\n",
    "        if p == 'DO':\n",
    "            # lower is worse for DO\n",
    "            high_mask = s < t.get('high', np.nan)\n",
    "            medium_mask = (s < t.get('medium', np.nan)) & (~high_mask)\n",
    "            low_mask = (~high_mask) & (~medium_mask) & s.notna()\n",
    "        else:\n",
    "            high_mask = s > t.get('high', np.nan)\n",
    "            medium_mask = (s > t.get('medium', np.nan)) & (~high_mask)\n",
    "            low_mask = (~high_mask) & (~medium_mask) & s.notna()\n",
    "\n",
    "        df.loc[high_mask, f'{p}_score'] = RISK_TO_SCORE['High']\n",
    "        df.loc[medium_mask, f'{p}_score'] = RISK_TO_SCORE['Medium']\n",
    "        df.loc[low_mask, f'{p}_score'] = RISK_TO_SCORE['Low']\n",
    "\n",
    "    # Convert score columns to nullable integers for clarity\n",
    "    score_cols = [f'{p}_score' for p in PARAMS]\n",
    "    for sc in score_cols:\n",
    "        df[sc] = df[sc].astype('Int64')  # keeps NaNs\n",
    "\n",
    "    # --- Aggregation: overall_score ---\n",
    "    if AGG_METHOD == 'max':\n",
    "        # worst-case: take max of scores (ignoring NaNs)\n",
    "        df['overall_score'] = df[score_cols].max(axis=1, skipna=True)\n",
    "    elif AGG_METHOD == 'weighted':\n",
    "        # weighted average across available scores then round to nearest integer score\n",
    "        weight_arr = np.array([WEIGHTS[p] for p in PARAMS], dtype=float)\n",
    "        scores_arr = df[score_cols].to_numpy(dtype=float)  # NaNs remain\n",
    "        # numerator: sum(score * weight) ignoring NaNs\n",
    "        numer = np.nansum(np.where(np.isnan(scores_arr), 0, scores_arr * weight_arr), axis=1)\n",
    "        denom = np.nansum(np.where(np.isnan(scores_arr), 0, weight_arr), axis=1)\n",
    "        # avoid division by zero: where denom==0 => set overall_score NaN\n",
    "        overall = np.where(denom > 0, numer / denom, np.nan)\n",
    "        # round to nearest integer score 0/1/2\n",
    "        overall_rounded = np.round(overall).astype('Int64')\n",
    "        df['overall_score'] = pd.Series(overall_rounded, index=df.index)\n",
    "\n",
    "    # Require minimum non-missing parameter scores\n",
    "    df['non_missing_scores'] = df[score_cols].notna().sum(axis=1)\n",
    "    df.loc[df['non_missing_scores'] < MIN_NON_MISSING, 'overall_score'] = pd.NA\n",
    "\n",
    "    # Map numeric overall_score back to Risk labels and make ordered categorical\n",
    "    df['Risk'] = df['overall_score'].map(SCORE_TO_RISK)\n",
    "    df['Risk'] = pd.Categorical(df['Risk'], categories=['Low', 'Medium', 'High'], ordered=True)\n",
    "\n",
    "    # --- Top driver(s): which parameter(s) had the overall_score ---\n",
    "    # Vectorized computation for top drivers (fast)\n",
    "    score_matrix = df[score_cols].to_numpy(dtype=float)  # shape (n_rows, n_params)\n",
    "    # Replace NaN with -inf to avoid selecting them as top drivers\n",
    "    nan_mask = np.isnan(score_matrix)\n",
    "    score_matrix_for_argmax = np.where(nan_mask, -9999.0, score_matrix)\n",
    "    # max per row\n",
    "    row_max = np.max(score_matrix_for_argmax, axis=1)\n",
    "    top_driver_list = []\n",
    "    for i, rm in enumerate(row_max):\n",
    "        if rm == -9999.0:\n",
    "            top_driver_list.append(pd.NA)\n",
    "            continue\n",
    "        drivers = [PARAMS[j] for j, val in enumerate(score_matrix_for_argmax[i]) if val == rm]\n",
    "        top_driver_list.append(\",\".join(drivers) if drivers else pd.NA)\n",
    "    df['top_driver'] = pd.Series(top_driver_list, index=df.index)\n",
    "\n",
    "    # --- Diagnostics & warnings ---\n",
    "    print(\"Per-parameter risk (score) counts:\")\n",
    "    for p in PARAMS:\n",
    "        counts = df[f'{p}_score'].value_counts(dropna=False).sort_index()\n",
    "        readable = { (SCORE_TO_RISK.get(int(k), 'NaN') if not pd.isna(k) else 'NaN'): v for k, v in counts.items() }\n",
    "        print(f\"  {p}: {readable}\")\n",
    "\n",
    "    print(\"\\nOverall Risk distribution (including NaN):\")\n",
    "    display(df['Risk'].value_counts(dropna=False))\n",
    "\n",
    "    # Data-quality warnings\n",
    "    for p in PARAMS:\n",
    "        pct_missing = df[p].isna().mean()\n",
    "        if pct_missing > MISSING_WARNING_PCT:\n",
    "            print(f\"Warning: {p} missing in {pct_missing:.1%} of rows.\")\n",
    "        high_pct = (df[f'{p}_score'] == RISK_TO_SCORE['High']).mean()\n",
    "        if high_pct > HIGH_PCT_THRESHOLD:\n",
    "            print(f\"Warning: {p} marked High in {high_pct:.1%} of rows — verify units/thresholds.\")\n",
    "\n",
    "    # Flag suspicious numeric ranges (example: pH outside 0-14)\n",
    "    if 'pH' in df.columns:\n",
    "        bad_ph_count = (~df['pH'].between(0, 14) & df['pH'].notna()).sum()\n",
    "        if bad_ph_count > 0:\n",
    "            print(f\"Warning: {bad_ph_count} pH values outside 0-14 range. Inspect these rows manually.\")\n",
    "\n",
    "    # Optionally drop intermediate columns to keep dataframe clean\n",
    "    if not KEEP_INTERMEDIATE:\n",
    "        drop_cols = score_cols + [f'{p}_missing' for p in PARAMS] + ['non_missing_scores']\n",
    "        df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "\n",
    "    # finished\n",
    "    print(\"\\nRisk computation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0091f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 4 rows with missing Risk labels before training.\n",
      "Encoded classes: ['High', 'Low', 'Medium']\n",
      "Prepared X and y_encoded:\n",
      "X shape: (1987, 8)\n",
      "y shape: (1987,)\n"
     ]
    }
   ],
   "source": [
    "# ==== DATA PREPARATION BEFORE TRAINING ====\n",
    "\n",
    "# 1. Remove rows with missing Risk labels\n",
    "missing_before = df['Risk'].isna().sum()\n",
    "if missing_before > 0:\n",
    "    print(f\"Dropping {missing_before} rows with missing Risk labels before training.\")\n",
    "    df = df[df['Risk'].notna()].reset_index(drop=True)\n",
    "\n",
    "# 2. Encode target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['Risk'])\n",
    "print(\"Encoded classes:\", list(le.classes_))\n",
    "\n",
    "# 3. Define features\n",
    "features = ['Temp','DO','pH','Conductivity','BOD','Nitrate','FecalColiform','TotalColiform']\n",
    "features = [f for f in features if f in df.columns]\n",
    "\n",
    "# 4. Build X DataFrame\n",
    "X = df[features].copy()\n",
    "\n",
    "print(\"Prepared X and y_encoded:\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ae299aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: {0: 1203, 2: 714, 1: 70}\n",
      "Using CV_FOLDS = 5\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Accuracy: 0.9849246231155779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      0.99      1.00       241\n",
      "         Low       0.92      0.79      0.85        14\n",
      "      Medium       0.97      0.99      0.98       143\n",
      "\n",
      "    accuracy                           0.98       398\n",
      "   macro avg       0.96      0.92      0.94       398\n",
      "weighted avg       0.98      0.98      0.98       398\n",
      "\n",
      "Saved model to: /Users/anishsharma/Developer/SE project/extract/rf_water_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# ==== TRAINING CELL (clean, minimal, final) ====\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "CV_FOLDS = 5\n",
    "N_ITER = 20\n",
    "\n",
    "# X must be a DataFrame and y must be encoded before this cell\n",
    "numeric_cols = ['Temp','DO','pH','Conductivity','BOD','Nitrate','FecalColiform','TotalColiform']\n",
    "numeric_cols = [c for c in numeric_cols if c in X.columns]\n",
    "categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "# --- Preprocessor ---\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('impute', SimpleImputer(strategy='median')),\n",
    "        ('scale', StandardScaler())\n",
    "    ]), numeric_cols),\n",
    "    \n",
    "    ('cat', Pipeline([\n",
    "        ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ]), categorical_cols)\n",
    "])\n",
    "\n",
    "# --- Full pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# --- Hyperparameter search ---\n",
    "param_dist = {\n",
    "    'clf__n_estimators': [100, 200, 300, 500],\n",
    "    'clf__max_depth': [None, 8, 12, 20],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'clf__max_features': ['sqrt', 'log2', 0.5],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Compute minimum class size and adjust CV folds\n",
    "class_counts = pd.Series(y_encoded).value_counts()\n",
    "min_count = int(class_counts.min())\n",
    "print(\"Class counts:\", class_counts.to_dict())\n",
    "\n",
    "CV_FOLDS = min(5, max(2, min_count))  # adaptive CV folds\n",
    "print(\"Using CV_FOLDS =\", CV_FOLDS)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=N_ITER,\n",
    "    scoring='f1_macro',\n",
    "    cv=cv,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# --- Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_encoded\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# --- Eval ---\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=list(le.classes_)))\n",
    "\n",
    "# --- Save artifact ---\n",
    "artifact = {\n",
    "    'pipeline': best_model,\n",
    "    'label_encoder': le,\n",
    "    'features': list(X.columns),\n",
    "    'best_params': search.best_params_\n",
    "}\n",
    "\n",
    "joblib.dump(artifact, MODEL_OUTPUT, compress=3)\n",
    "print(\"Saved model to:\", MODEL_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c0aea1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline loaded. Classes: ['High' 'Low' 'Medium']\n",
      "Pred: ['High']\n",
      "0: 98.5915%\n",
      "1: 0.1380%\n",
      "2: 1.2705%\n"
     ]
    }
   ],
   "source": [
    "# test examples\n",
    "\n",
    "artifact = joblib.load(MODEL_OUTPUT)\n",
    "pipeline = artifact['pipeline']\n",
    "le = artifact['label_encoder']\n",
    "features = artifact['features']\n",
    "\n",
    "# 1) Sanity: shape and classes\n",
    "print(\"Pipeline loaded. Classes:\", le.classes_)\n",
    "\n",
    "# 2) Single sample prediction and probabilities\n",
    "sample = X_test.iloc[[0]][features]  # use a real row\n",
    "pred = pipeline.predict(sample)[0]\n",
    "proba = pipeline.predict_proba(sample)[0]\n",
    "print(\"Pred:\", le.inverse_transform([pred]) if hasattr(le, 'inverse_transform') else pred)\n",
    "for cls, p in zip(pipeline.classes_, proba):\n",
    "    print(f\"{cls}: {p*100:.4f}%\")\n",
    "\n",
    "# 3) Batch shape test\n",
    "batch = X_test.iloc[:5][features]\n",
    "assert pipeline.predict_proba(batch).shape == (5, len(pipeline.classes_))\n",
    "\n",
    "# 4) Missing value handling test (should not raise)\n",
    "test_missing = batch.copy()\n",
    "test_missing.iloc[0, :] = np.nan\n",
    "_ = pipeline.predict_proba(test_missing)  # should run without crashing if pipeline handles missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d13e9fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact type: <class 'dict'>\n",
      "Artifact keys: ['pipeline', 'label_encoder', 'features', 'best_params']\n",
      "Using artifact['pipeline'].\n",
      "\n",
      "Ready to run quick tests.\n",
      "Pipeline type: <class 'sklearn.pipeline.Pipeline'>\n",
      "Number of features expected: 8\n",
      "\n",
      "Single-sample prediction (raw): 0\n",
      "Single-sample prediction (label): High\n",
      "\n",
      "Probabilities:\n",
      " 0: 98.5915%\n",
      " 1: 0.1380%\n",
      " 2: 1.2705%\n",
      "\n",
      "Batch predict_proba shape: (5, 3)\n",
      "Missing-value handling test: OK (no exception).\n"
     ]
    }
   ],
   "source": [
    "# Some gpt test cell\n",
    "\n",
    "# Robust loader + tester for saved model artifact\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load artifact\n",
    "artifact = joblib.load(MODEL_OUTPUT)\n",
    "print(\"Artifact type:\", type(artifact))\n",
    "\n",
    "# If artifact is a dict-like, print keys\n",
    "if isinstance(artifact, dict):\n",
    "    print(\"Artifact keys:\", list(artifact.keys()))\n",
    "else:\n",
    "    # Not a dict - print repr and try to treat as pipeline directly\n",
    "    print(\"Artifact is not a dict. repr:\")\n",
    "    print(repr(artifact))\n",
    "\n",
    "# Attempt to locate pipeline / model / components\n",
    "pipeline = None\n",
    "le = None\n",
    "features = None\n",
    "\n",
    "if isinstance(artifact, dict):\n",
    "    # Common variants handled:\n",
    "    # 1) {'pipeline': <Pipeline>, 'label_encoder': le, 'features': [...]}\n",
    "    if 'pipeline' in artifact:\n",
    "        pipeline = artifact['pipeline']\n",
    "        le = artifact.get('label_encoder', None)\n",
    "        features = artifact.get('features', None)\n",
    "        print(\"Using artifact['pipeline'].\")\n",
    "    # 2) {'model': <clf>, 'imputer': <preproc>, 'label_encoder': le, 'features': [...]}\n",
    "    elif 'model' in artifact:\n",
    "        model = artifact['model']\n",
    "        imputer = artifact.get('imputer', None) or artifact.get('preprocessor', None)\n",
    "        le = artifact.get('label_encoder', None) or artifact.get('le', None)\n",
    "        features = artifact.get('features', None) or artifact.get('feature_names', None)\n",
    "        if imputer is not None:\n",
    "            # build a small pipeline (imputer might be a transformer; if it's a bare SimpleImputer it's still fine)\n",
    "            pipeline = Pipeline([('preproc', imputer), ('clf', model)])\n",
    "            print(\"Rebuilt pipeline from artifact['imputer'] + artifact['model'].\")\n",
    "        else:\n",
    "            # no preprocessor: create a minimal pipeline that just contains the model\n",
    "            pipeline = Pipeline([('clf', model)])\n",
    "            print(\"Created pipeline from artifact['model'] (no imputer found).\")\n",
    "    # 3) older saving: saved classifier directly (not in dict)\n",
    "    else:\n",
    "        # check for common alternatives\n",
    "        possible_le_keys = [k for k in artifact.keys() if 'label' in k.lower() or 'le' == k.lower()]\n",
    "        if possible_le_keys:\n",
    "            print(\"Found potential label encoder keys:\", possible_le_keys)\n",
    "        # try to find model-like object\n",
    "        # fallback: if dict contains only one sklearn estimator, try to use it\n",
    "        # not doing that automatically to avoid speculation\n",
    "        print(\"Unhandled artifact dict layout. Please examine keys above.\")\n",
    "else:\n",
    "    # artifact not a dict: maybe it is directly the pipeline/model\n",
    "    try:\n",
    "        # If it looks like a pipeline or estimator, use it directly\n",
    "        from sklearn.base import BaseEstimator\n",
    "        if isinstance(artifact, BaseEstimator) or hasattr(artifact, 'predict'):\n",
    "            pipeline = artifact\n",
    "            print(\"Artifact appears to be a model/pipeline object and will be used directly.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# At this point, pipeline may or may not be set\n",
    "if pipeline is None:\n",
    "    raise KeyError(\"Could not find a pipeline/model in the artifact automatically. \"\n",
    "                   \"Please inspect the artifact keys printed above and let me know its structure, \"\n",
    "                   \"or re-save your artifact as: joblib.dump({'pipeline': pipeline, 'label_encoder': le, 'features': features}, MODEL_OUTPUT)\")\n",
    "\n",
    "# If features missing, try to infer from saved metadata or ask user fallback\n",
    "if features is None:\n",
    "    # try to infer from pipeline if it has named transformer with get_feature_names_out\n",
    "    try:\n",
    "        if hasattr(pipeline, 'named_steps') and 'preproc' in pipeline.named_steps:\n",
    "            pre = pipeline.named_steps['preproc']\n",
    "            if hasattr(pre, 'get_feature_names_out'):\n",
    "                features = list(pre.get_feature_names_out())\n",
    "                print(\"Inferred feature names from pipeline.preproc.get_feature_names_out().\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if features is None:\n",
    "    # fallback: try artifact['features'] variations, else use X_test columns if available\n",
    "    if isinstance(artifact, dict) and 'features' in artifact:\n",
    "        features = artifact['features']\n",
    "    elif 'X_test' in globals():\n",
    "        print(\"Using X_test columns as features (ensure X_test exists and is correct).\")\n",
    "        features = list(X_test.columns)\n",
    "    else:\n",
    "        print(\"Features not found. You must supply `features` list or create X_test in the session.\")\n",
    "        raise KeyError(\"Missing 'features' information. Provide artifact['features'] or set X_test in the session.\")\n",
    "\n",
    "# Try to extract label encoder if None\n",
    "if le is None:\n",
    "    if isinstance(artifact, dict):\n",
    "        for k in artifact.keys():\n",
    "            if 'label' in k.lower() or k.lower() in ('le', 'label_encoder', 'labelencoder'):\n",
    "                le = artifact[k]\n",
    "                print(f\"Using label encoder from artifact['{k}'].\")\n",
    "                break\n",
    "\n",
    "# Now we have pipeline, features, possibly le. Run the same tests as before\n",
    "print(\"\\nReady to run quick tests.\")\n",
    "print(\"Pipeline type:\", type(pipeline))\n",
    "print(\"Number of features expected:\", len(features))\n",
    "\n",
    "# Create X_test if not present\n",
    "if 'X_test' not in globals():\n",
    "    print(\"X_test not found in memory. Creating a dummy X_test with NaNs for feature-shape checks.\")\n",
    "    X_test = pd.DataFrame([ {f: np.nan for f in features} ])\n",
    "\n",
    "# Single-sample prediction\n",
    "sample = X_test.iloc[[0]][features]\n",
    "pred = pipeline.predict(sample)[0]\n",
    "try:\n",
    "    pred_label = le.inverse_transform([pred])[0] if (le is not None and hasattr(le, 'inverse_transform')) else pred\n",
    "except Exception:\n",
    "    pred_label = pred\n",
    "\n",
    "print(\"\\nSingle-sample prediction (raw):\", pred)\n",
    "print(\"Single-sample prediction (label):\", pred_label)\n",
    "\n",
    "# Probabilities (if available)\n",
    "if hasattr(pipeline, \"predict_proba\"):\n",
    "    proba = pipeline.predict_proba(sample)[0]\n",
    "    cls_names = getattr(pipeline, \"classes_\", getattr(le, \"classes_\", None))\n",
    "    if cls_names is None:\n",
    "        cls_names = list(range(len(proba)))\n",
    "    print(\"\\nProbabilities:\")\n",
    "    for cls, p in zip(cls_names, proba):\n",
    "        print(f\" {cls}: {p*100:.4f}%\")\n",
    "else:\n",
    "    print(\"Pipeline has no predict_proba method. Skipping probabilities.\")\n",
    "\n",
    "# Batch test\n",
    "batch = X_test.iloc[:5][features]\n",
    "if hasattr(pipeline, \"predict_proba\"):\n",
    "    probs_batch = pipeline.predict_proba(batch)\n",
    "    print(\"\\nBatch predict_proba shape:\", probs_batch.shape)\n",
    "else:\n",
    "    print(\"Batch predict_proba skipped (no predict_proba).\")\n",
    "\n",
    "# Missing-value test\n",
    "test_missing = batch.copy()\n",
    "test_missing.iloc[0, :] = np.nan\n",
    "try:\n",
    "    _ = pipeline.predict_proba(test_missing) if hasattr(pipeline, \"predict_proba\") else pipeline.predict(test_missing)\n",
    "    print(\"Missing-value handling test: OK (no exception).\")\n",
    "except Exception as e:\n",
    "    print(\"Missing-value handling test: FAILED. Exception:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5505511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a real row from df for prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to run a realistic single-row prediction with precise percentage output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load artifact if not already loaded\n",
    "if 'artifact' not in globals():\n",
    "    artifact = joblib.load(MODEL_OUTPUT)\n",
    "\n",
    "# Reconstruct pipeline if not present\n",
    "if 'pipeline' not in globals():\n",
    "    if 'model' in artifact and ('imputer' in artifact or 'preprocessor' in artifact):\n",
    "        imputer = artifact.get('imputer', artifact.get('preprocessor'))\n",
    "        pipeline = Pipeline([('preproc', imputer), ('clf', artifact['model'])])\n",
    "    elif isinstance(artifact, dict) and 'model' in artifact:\n",
    "        pipeline = Pipeline([('clf', artifact['model'])])\n",
    "    else:\n",
    "        pipeline = artifact  # fallback if artifact is pipeline\n",
    "\n",
    "le = artifact.get('label_encoder', artifact.get('le', None))\n",
    "features = artifact.get('features', None)\n",
    "if features is None:\n",
    "    raise KeyError(\"Artifact missing 'features'. Provide artifact['features'] or set features list.\")\n",
    "\n",
    "# Create a realistic test sample:\n",
    "# 1) Prefer to use a real row from the cleaned df if available\n",
    "if 'df' in globals() and set(features).issubset(set(df.columns)):\n",
    "    # pick a real non-NaN row if possible\n",
    "    sample_row = df.dropna(subset=features).sample(n=1, random_state=42).iloc[0][features]\n",
    "    sample_df = pd.DataFrame([sample_row.values], columns=features)\n",
    "    print(\"Using a real row from df for prediction.\")\n",
    "else:\n",
    "    # 2) fallback: build a median-based sample so values are numeric and sensible\n",
    "    print(\"df not available or missing features — constructing median-based sample.\")\n",
    "    sample_values = {}\n",
    "    for f in features:\n",
    "        if f in globals().get('df', pd.DataFrame()).columns:\n",
    "            sample_values[f] = df[f].median(skipna=True)\n",
    "        else:\n",
    "            # reasonable generic defaults (tweak if you want)\n",
    "            if 'pH' in f.lower():\n",
    "                sample_values[f] = 7.2\n",
    "            elif 'do' in f.lower():\n",
    "                sample_values[f] = 5.0\n",
    "            elif 'bod' in f.lower():\n",
    "                sample_values[f] = 1.0\n",
    "            elif 'conduct' in f.lower():\n",
    "                sample_values[f] = 200.0\n",
    "            elif 'nitrate' in f.lower():\n",
    "                sample_values[f] = 2.0\n",
    "            elif 'coliform' in f.lower():\n",
    "                sample_values[f] = 50.0\n",
    "            else:\n",
    "                sample_values[f] = 0.0\n",
    "    sample_df = pd.DataFrame([sample_values], columns=features)\n",
    "\n",
    "# Ensure the DataFrame has exact feature names and order\n",
    "sample_df = sample_df[features]\n",
    "\n",
    "# Run prediction\n",
    "# --- Run prediction safely (no warnings, preserves feature names if possible) ---\n",
    "preproc = pipeline.named_steps.get('preproc')\n",
    "clf = pipeline.named_steps.get('clf')\n",
    "\n",
    "try:\n",
    "    # if the preprocessor supports get_feature_names_out, we build a named DataFrame\n",
    "    transformed_cols = preproc.get_feature_names_out()\n",
    "    X_trans = preproc.transform(sample_df)\n",
    "    X_trans_df = pd.DataFrame(X_trans, columns=transformed_cols, index=sample_df.index)\n",
    "\n",
    "    # direct classifier prediction (no feature-name warnings)\n",
    "    probs = clf.predict_proba(X_trans_df)[0]\n",
    "    pred_raw = clf.predict(X_trans_df)[0]\n",
    "\n",
    "except Exception as e:\n",
    "    # fallback to pipeline.predict_proba if preproc does NOT support named columns\n",
    "    print(\"Could not create named DataFrame from preprocessor (fallback). Error:\", e)\n",
    "    probs = pipeline.predict_proba(sample_df)[0]\n",
    "    pred_raw = pipeline.predict(sample_df)[0]\n",
    "\n",
    "# decode label\n",
    "try:\n",
    "    pred_label = le.inverse_transform([pred_raw])[0]\n",
    "except Exception:\n",
    "    pred_label = pred_raw\n",
    "\n",
    "# get human-readable class names\n",
    "encoded_classes = getattr(pipeline, \"classes_\", None)\n",
    "if le is not None and encoded_classes is not None:\n",
    "    try:\n",
    "        readable_classes = le.inverse_transform(encoded_classes)\n",
    "    except Exception:\n",
    "        readable_classes = encoded_classes\n",
    "else:\n",
    "    readable_classes = encoded_classes if encoded_classes is not None else [f\"class_{i}\" for i in range(len(probs))]\n",
    "    # Print precise percentages\n",
    "    print(\"\\nPrediction result:\")\n",
    "    print(\" Predicted label (raw):\", pred_raw)\n",
    "    print(\" Predicted label (decoded):\", pred_label)\n",
    "    print(\"\\nProbabilities (precise percentages):\")\n",
    "    for cls_name, p in zip(readable_classes, probs):\n",
    "        print(f\" {cls_name}: {p*100:.4f}%\")\n",
    "    print(\"\\nPredicted probability (top):\", round(100.0 * probs.max(), 4), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c5db65a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded artifact keys: ['pipeline', 'label_encoder', 'features', 'best_params']\n",
      "Using artifact['pipeline'].\n",
      "\n",
      "Prediction result:\n",
      " Predicted (raw): 2\n",
      " Predicted (decoded): Medium\n",
      "\n",
      "Probabilities (precise percentages):\n",
      " High: 8.4022%\n",
      " Low: 5.0351%\n",
      " Medium: 86.5628%\n",
      "\n",
      "Predicted probability (top): 86.5628 %\n"
     ]
    }
   ],
   "source": [
    "# Robust prediction cell — replace your old BLANK_INPUT cell with this\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# put your sample values here\n",
    "BLANK_INPUT = {\n",
    "    'Temp': 29.5,\n",
    "    'DO': 5.8,\n",
    "    'pH': 7.2,\n",
    "    'Conductivity': 150,\n",
    "    'BOD': 2.0,\n",
    "    'Nitrate': 0.5,\n",
    "    'FecalColiform': 120,\n",
    "    'TotalColiform': 900\n",
    "}\n",
    "\n",
    "if not Path(MODEL_OUTPUT).exists():\n",
    "    raise FileNotFoundError(f\"Model artifact not found at {MODEL_OUTPUT}. Run training cell first.\")\n",
    "\n",
    "obj = joblib.load(MODEL_OUTPUT)\n",
    "print(\"Loaded artifact keys:\", list(obj.keys()) if isinstance(obj, dict) else \"artifact is not a dict\")\n",
    "\n",
    "# Try to get pipeline, label encoder, features in a robust way\n",
    "pipeline = None\n",
    "le = None\n",
    "feat_list = None\n",
    "\n",
    "if isinstance(obj, dict):\n",
    "    # preferred modern structure: 'pipeline' saved\n",
    "    if 'pipeline' in obj:\n",
    "        pipeline = obj['pipeline']\n",
    "        le = obj.get('label_encoder', obj.get('le', None))\n",
    "        feat_list = obj.get('features', None)\n",
    "        print(\"Using artifact['pipeline'].\")\n",
    "    # older structure: 'model' + 'imputer'\n",
    "    elif 'model' in obj and ('imputer' in obj or 'preprocessor' in obj):\n",
    "        model = obj['model']\n",
    "        imputer = obj.get('imputer', obj.get('preprocessor'))\n",
    "        try:\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            pipeline = Pipeline([('preproc', imputer), ('clf', model)])\n",
    "            print(\"Rebuilt pipeline from artifact['imputer'] + artifact['model'].\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Failed to rebuild pipeline from model+imputer: \" + str(e))\n",
    "        le = obj.get('label_encoder', obj.get('le', None))\n",
    "        feat_list = obj.get('features', None)\n",
    "    # artifact saved as single model object in dict (rare)\n",
    "    elif 'model' in obj:\n",
    "        pipeline = obj['model']\n",
    "        le = obj.get('label_encoder', obj.get('le', None))\n",
    "        feat_list = obj.get('features', None)\n",
    "        print(\"Using artifact['model'] directly.\")\n",
    "    else:\n",
    "        # fallback: maybe artifact contains pipeline under some other name\n",
    "        # try to detect the first sklearn estimator-looking object\n",
    "        possible_keys = [k for k, v in obj.items() if hasattr(v, 'predict')]\n",
    "        if possible_keys:\n",
    "            pipeline = obj[possible_keys[0]]\n",
    "            le = obj.get('label_encoder', obj.get('le', None))\n",
    "            feat_list = obj.get('features', None)\n",
    "            print(f\"Using artifact['{possible_keys[0]}'] as pipeline (fallback).\")\n",
    "else:\n",
    "    # not a dict; maybe the artifact *is* the pipeline\n",
    "    if hasattr(obj, 'predict'):\n",
    "        pipeline = obj\n",
    "        print(\"Artifact is a model/pipeline object directly.\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Loaded artifact format not recognized. Please re-save artifact with {'pipeline': pipeline, 'label_encoder': le, 'features': features}.\")\n",
    "\n",
    "# sanity checks\n",
    "if pipeline is None:\n",
    "    raise RuntimeError(\"Could not extract pipeline from artifact. Inspect artifact structure.\")\n",
    "\n",
    "# Get features list; if missing try pipeline/preprocessor or user-provided X\n",
    "if feat_list is None:\n",
    "    # try to get from pipeline.preproc if available\n",
    "    try:\n",
    "        pre = pipeline.named_steps.get('preproc', None)\n",
    "        if pre is not None and hasattr(pre, 'get_feature_names_out'):\n",
    "            feat_list = list(pre.get_feature_names_out())\n",
    "            print(\"Inferred feature names from preprocessor.get_feature_names_out().\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if feat_list is None:\n",
    "    # as last resort try existing df (if available)\n",
    "    if 'df' in globals():\n",
    "        feat_list = [c for c in BLANK_INPUT.keys() if c in df.columns]\n",
    "        if not feat_list:\n",
    "            # use df numeric feature intersection\n",
    "            feat_list = list(df.columns)\n",
    "            print(\"Fallback: using df.columns as features (verify ordering).\")\n",
    "    else:\n",
    "        raise KeyError(\"features list not found in artifact and df not available. Provide artifact['features'] or set feat_list manually.\")\n",
    "\n",
    "# Build input DataFrame with exact ordering\n",
    "x_new = pd.DataFrame([BLANK_INPUT])\n",
    "# Add any missing columns from feat_list with NaN\n",
    "for c in feat_list:\n",
    "    if c not in x_new.columns:\n",
    "        x_new[c] = np.nan\n",
    "# Reindex into the exact order\n",
    "x_new = x_new.reindex(columns=feat_list)\n",
    "\n",
    "# Run prediction using pipeline (which handles preprocessing)\n",
    "# Suppress the benign sklearn warning about feature names if it appears — optional\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "    # predict probabilities if available\n",
    "    if hasattr(pipeline, \"predict_proba\"):\n",
    "        probs = pipeline.predict_proba(x_new)[0]\n",
    "    else:\n",
    "        probs = None\n",
    "    pred_raw = pipeline.predict(x_new)[0]\n",
    "\n",
    "# Decode predicted label\n",
    "pred_label = None\n",
    "if le is not None:\n",
    "    try:\n",
    "        # label encoder expects array-like numeric labels or encoded labels\n",
    "        pred_label = le.inverse_transform([pred_raw])[0]\n",
    "    except Exception:\n",
    "        try:\n",
    "            # sometimes pipeline.predict already returns decoded strings\n",
    "            pred_label = pred_raw\n",
    "        except Exception:\n",
    "            pred_label = pred_raw\n",
    "else:\n",
    "    # try pipeline.classes_ -> maybe they are string labels already\n",
    "    if hasattr(pipeline, \"classes_\"):\n",
    "        classes = pipeline.classes_\n",
    "        try:\n",
    "            pred_label = classes[list(classes).index(pred_raw)] if pred_raw in classes else pred_raw\n",
    "        except Exception:\n",
    "            pred_label = pred_raw\n",
    "    else:\n",
    "        pred_label = pred_raw\n",
    "\n",
    "# Print results\n",
    "print(\"\\nPrediction result:\")\n",
    "print(\" Predicted (raw):\", pred_raw)\n",
    "print(\" Predicted (decoded):\", pred_label)\n",
    "\n",
    "if probs is not None:\n",
    "    # find readable class names\n",
    "    class_names = getattr(pipeline, \"classes_\", None)\n",
    "    if le is not None and class_names is not None:\n",
    "        try:\n",
    "            readable = le.inverse_transform(class_names)\n",
    "        except Exception:\n",
    "            readable = class_names\n",
    "    else:\n",
    "        readable = class_names if class_names is not None else [f\"class_{i}\" for i in range(len(probs))]\n",
    "\n",
    "    print(\"\\nProbabilities (precise percentages):\")\n",
    "    for cls_name, p in zip(readable, probs):\n",
    "        print(f\" {cls_name}: {p*100:.4f}%\")\n",
    "    print(\"\\nPredicted probability (top):\", round(100.0 * np.max(probs), 4), \"%\")\n",
    "else:\n",
    "    print(\"\\nNo predict_proba available for this model. Only label predicted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
